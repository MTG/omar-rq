# general training parameters
train.wandb_params = {
    "project": "mtg-ssl",
    "name": "mask_conformer_rope_multi4_encodec",
    "offline": True,
    # NOTE: path to logs in the BSC cluster. Change it for local experiments
    "save_dir": "/gpfs/projects/upf97/logs/",
    "entity": "mtg-upf",
    "group": "",
}

# modules to use
build_module.representation = @nets.encodec.EnCodec
build_module.module = @modules.maskingmodel.MaskingModel
build_module.net = @nets.conformer.Conformer

# Choose the devalopment dataloader
build_dev_datamodule.datamodule = @discotube

# Lighting trainer parameters
train.params = {
    "accelerator": "gpu",
    "devices": 4,
    "max_steps": 400000,
    "log_every_n_steps": 50,
    "precision": "bf16-mixed",
    "strategy": "ddp_find_unused_parameters_true",
    "num_sanity_val_steps": 0
}

# Dataloader
AudioDataset.num_frames = 480000 # 30s
AudioDataset.orig_freq = 16000
AudioDataset.new_freq = 24000
AudioDataset.mono = True
AudioDataset.half_precision = False
AudioDataModule.num_workers = 80

# Discogs datamodule parameters
DiscotubeAudioDataModule.batch_size = 128
DiscotubeAudioDataModule.data_dir = "/gpfs/scratch/upf97/mmap/"
DiscotubeAudioDataModule.filelist_train = "/gpfs/projects/upf97/data/train_mmap.txt"
DiscotubeAudioDataModule.filelist_val = "/gpfs/projects/upf97/data/test_mmap.txt"

# CosineAnnealing scheduler
CosineAnnealingCallback.warmup_steps = 30000
CosineAnnealingCallback.eta_min = 1e-7

# Encodec parameters
nets.encodec.EnCodec.weights_path = "/gpfs/scratch/upf97/model_weights/encodec_24khz/"

# MaskingModel parameters
modules.maskingmodel.MaskingModel.num_codebooks = 4
modules.maskingmodel.MaskingModel.lr = 1e-4
modules.maskingmodel.MaskingModel.weight_decay = 1e-2
modules.maskingmodel.MaskingModel.codebook_size = 8192
modules.maskingmodel.MaskingModel.codebook_dim = 16
modules.maskingmodel.MaskingModel.mask_seconds = 0.4
modules.maskingmodel.MaskingModel.mask_prob = 0.6
modules.maskingmodel.MaskingModel.seed = 0
modules.maskingmodel.MaskingModel.plot_tokens = False

# Transformer parameters
nets.conformer.Conformer.patch_size = (128, 5)  # 66.6ms
nets.conformer.Conformer.embed_dim = 512
nets.conformer.Conformer.depth = 12
nets.conformer.Conformer.conv_kernel_size = 5
nets.conformer.Conformer.num_heads = 8
nets.conformer.Conformer.mlp_ratio = 4.0
nets.conformer.Conformer.mlp_residual_factor = 4.0
nets.conformer.Conformer.dropout = 0.1
nets.conformer.Conformer.use_deepnorm = True
nets.conformer.Conformer.alpha_deepnorm = 2.21 # we can tune this number
nets.conformer.Conformer.beta_deepnorm = 0.0026 # we can tune this number
nets.conformer.Conformer.use_rope = True
