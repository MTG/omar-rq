dataset_name = "magnatagatune"
embeddings_dir = "/gpfs/scratch/upf97/embeddings/"

# Lighting Trainer parameters, overwrites the training config
predict.device_dict = {
	"accelerator": "gpu",
	"devices": 1,
}

# Embedding taking location from the neural network
predict.embedding_layer = [6]
predict.overlap_ratio = 0.5

predict.embeddings_dir = %embeddings_dir
predict.dataset_name = %dataset_name

# Audio Loader for embedding extraction
AudioEmbeddingDataModule.data_dir = "/data0/palonso/ssl-mtg/downstream_datasets/magnatagatune/"
AudioEmbeddingDataModule.file_format = "mp3"
AudioEmbeddingDataModule.orig_freq = 16000
AudioEmbeddingDataModule.new_freq = 16000 # TODO read from train cfg
AudioEmbeddingDataModule.mono = True # TODO read from train cfg
AudioEmbeddingDataModule.half_precision = True # TODO read from train cfg
AudioEmbeddingDataModule.num_workers = 20
AudioEmbeddingDataModule.batch_size = 64
AudioEmbeddingDataModule.overlap_ratio = 0.5
AudioEmbeddingDataModule.num_frames = 480000


build_module_and_datamodule.dataset_name = %dataset_name
build_module_and_datamodule.embeddings_dir = %embeddings_dir

MTTEmbeddingLoadingDataModule.gt_path = "/data0/palonso/ssl-mtg/downstream_datasets/magnatagatune/metadata/mtat/binary.npy"
MTTEmbeddingLoadingDataModule.train_filelist = "/data0/palonso/ssl-mtg/downstream_datasets/magnatagatune/metadata/mtat/train.npy"
MTTEmbeddingLoadingDataModule.val_filelist = "/data0/palonso/ssl-mtg/downstream_datasets/magnatagatune/metadata/mtat/valid.npy"
MTTEmbeddingLoadingDataModule.test_filelist = "/data0/palonso/ssl-mtg/downstream_datasets/magnatagatune/metadata/mtat/test.npy"
MTTEmbeddingLoadingDataModule.batch_size = 64
MTTEmbeddingLoadingDataModule.num_workers = 10
MTTEmbeddingLoadingDataModule.layer_aggregation = "none"
MTTEmbeddingLoadingDataModule.granularity = "chunk"
MTTEmbeddingLoadingDataModule.time_aggregation = "mean"

train_probe.wandb_params = {
    "project": "magnatagatune",
    "offline": True,
    "entity": "mtg-upf",
    "save_dir": "/gpfs/projects/upf97/logs/",
}

train_probe.train_params = {
    "accelerator": "gpu",
    "devices": 1,
    "log_every_n_steps": 50,
    "max_steps": 20000,
    "num_sanity_val_steps": 0,
    "check_val_every_n_epoch": 3,
}
train_probe.monitor = "val-MAP-macro"
train_probe.monitor_mode = "max"

optimize_probe.bound_conditions = {
    "hidden_size": (64, 1024),
    "dropout": (0.0, 0.5),
    "lr": (1e-5, 1e-3),
}
# Other parameters to optimize:
# "max_epochs": (10, 100)
# "batch_size": (32, 128)

optimize_probe.optim_process = False
optimize_probe.init_points = 5
optimize_probe.n_iter = 50
optimize_probe.seed = 1

# Warning: these parameters are ignored when the the Bayesian optimization is enabled
SequenceMultiLabelClassificationProbe.num_layers = 2
SequenceMultiLabelClassificationProbe.hidden_size = 512
SequenceMultiLabelClassificationProbe.dropout = 0.2
SequenceMultiLabelClassificationProbe.lr = 0.0001

SequenceMultiLabelClassificationProbe.activation = "relu"
SequenceMultiLabelClassificationProbe.bias = True
SequenceMultiLabelClassificationProbe.num_labels = 50
SequenceMultiLabelClassificationProbe.labels = "/gpfs/projects/upf97/downstream_datasets/magnatagatune/metadata/mtat/tags.npy"

# CosineAnnealing scheduler
CosineAnnealingCallback.warmup_steps = 2000
CosineAnnealingCallback.eta_min = 1e-7
