dataset_name = "beattracking"
embeddings_dir = "/data0/palonso/ssl-mtg/embeddings/"

# Lighting Trainer parameters, overwrites the training config
predict.device_dict = {
	"accelerator": "gpu",
	"devices": 1,
}

# Embedding taking location from the neural network
predict.embedding_layer = [6]
predict.overlap_ratio = 0.5

predict.embeddings_dir = %embeddings_dir
predict.dataset_name = %dataset_name

# Audio Loader for embedding extraction
AudioEmbeddingDataModule.data_dir = "/data0/palonso/ssl-mtg/downstream_datasets/beattracking/"
AudioEmbeddingDataModule.file_format = "wav"
AudioEmbeddingDataModule.num_workers = 20
AudioEmbeddingDataModule.batch_size = 32
AudioEmbeddingDataModule.overlap_ratio = 0.5
AudioEmbeddingDataModule.n_seconds = 30
# generate a padded last chunk if it covers at least e.g., 10% of n_seconds.
AudioEmbeddingDataModule.last_chunk_ratio = 0.1  


build_module_and_datamodule.dataset_name = %dataset_name
build_module_and_datamodule.embeddings_dir = %embeddings_dir

BeatTrackingEmbeddingLoadingDataModule.gt_path = "/data0/palonso/ssl-mtg/downstream_datasets/beattracking/groundtruth.pkl"
BeatTrackingEmbeddingLoadingDataModule.train_filelist = "/data0/palonso/ssl-mtg/downstream_datasets/beattracking/filelist_train.txt"
BeatTrackingEmbeddingLoadingDataModule.val_filelist = "/data0/palonso/ssl-mtg/downstream_datasets/beattracking/filelist_val.txt"
BeatTrackingEmbeddingLoadingDataModule.test_filelist = "/data0/palonso/ssl-mtg/downstream_datasets/beattracking/filelist_test.txt"
BeatTrackingEmbeddingLoadingDataModule.batch_size = 64
BeatTrackingEmbeddingLoadingDataModule.num_workers = 20
BeatTrackingEmbeddingLoadingDataModule.layer_aggregation = "none"
BeatTrackingEmbeddingLoadingDataModule.granularity = "frame"
BeatTrackingEmbeddingLoadingDataModule.time_aggregation = "none"

train_probe.wandb_params = {
    "project": "beattracking",
    "offline": False,
    "entity": "mtg-upf",
    "save_dir": "/data0/palonso/ssl-mtg/logs/",
}

train_probe.train_params = {
    "accelerator": "gpu",
    "devices": 1,
    "log_every_n_steps": 50,
    "max_steps": 20000,
    "num_sanity_val_steps": 5,
    "check_val_every_n_epoch": 1,
}
train_probe.monitor = "val-acc"
train_probe.monitor_mode = "max"

optimize_probe.bound_conditions = {
    "hidden_size": (64, 1024),
    "dropout": (0.0, 0.5),
    "lr": (1e-5, 1e-3),
}
# Other parameters to optimize:
# "max_epochs": (10, 100)
# "batch_size": (32, 128)

optimize_probe.optim_process = False
optimize_probe.init_points = 5
optimize_probe.n_iter = 50
optimize_probe.seed = 1

# Warning: these parameters are ignored when the the Bayesian optimization is enabled
SequenceClassificationProbe.num_layers = 2
SequenceClassificationProbe.hidden_size = 512
SequenceClassificationProbe.dropout = 0.2
SequenceClassificationProbe.lr = 0.0001

SequenceClassificationProbe.activation = "relu"
SequenceClassificationProbe.bias = True
SequenceClassificationProbe.num_labels = 1
SequenceClassificationProbe.labels = None

# CosineAnnealing scheduler
CosineAnnealingCallback.warmup_steps = 2000
CosineAnnealingCallback.eta_min = 1e-7
