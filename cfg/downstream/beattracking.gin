dataset_name = "beattracking"
embeddings_dir = "/data0/palonso/ssl-mtg/embeddings/"

# Lighting Trainer parameters, overwrites the training config
predict.device_dict = {
	"accelerator": "gpu",
	"devices": 1,
}

# Embedding taking location from the neural network
predict.embedding_layer = [6]
predict.overlap_ratio = 0.5

predict.embeddings_dir = %embeddings_dir
predict.dataset_name = %dataset_name

# Audio Loader for embedding extraction
AudioEmbeddingDataModule.data_dir = "/data0/palonso/ssl-mtg/downstream_datasets/beattracking/"
AudioEmbeddingDataModule.file_format = "wav"
AudioEmbeddingDataModule.num_workers = 20
AudioEmbeddingDataModule.batch_size = 32
AudioEmbeddingDataModule.overlap_ratio = 0.5
AudioEmbeddingDataModule.n_seconds = 30
# generate a padded last chunk if it covers at least e.g., 10% of n_seconds.
AudioEmbeddingDataModule.last_chunk_ratio = 0.1  


build_module_and_datamodule.dataset_name = %dataset_name
build_module_and_datamodule.embeddings_dir = %embeddings_dir

BeattrackingEmbeddingLoadingDataModule.gt_path = ""
BeattrackingEmbeddingLoadingDataModule.train_filelist = ""
BeattrackingEmbeddingLoadingDataModule.val_filelist = ""
BeattrackingEmbeddingLoadingDataModule.test_filelist = ""
BeattrackingEmbeddingLoadingDataModule.batch_size = 64
BeattrackingEmbeddingLoadingDataModule.num_workers = 10
BeattrackingEmbeddingLoadingDataModule.layer_aggregation = "none"
BeattrackingEmbeddingLoadingDataModule.granularity = "chunk"
BeattrackingEmbeddingLoadingDataModule.time_aggregation = "mean"

train_probe.wandb_params = {
    "project": "beattracking",
    "offline": False,
    "entity": "mtg-upf",
    "save_dir": "/data0/palonso/ssl-mtg/logs/",
}

train_probe.train_params = {
    "accelerator": "gpu",
    "devices": 1,
    "log_every_n_steps": 50,
    "max_steps": 20000,
    "num_sanity_val_steps": 0,
    "check_val_every_n_epoch": 1,
}
train_probe.monitor = "val-MAP-macro"
train_probe.monitor_mode = "max"

optimize_probe.bound_conditions = {
    "hidden_size": (64, 1024),
    "dropout": (0.0, 0.5),
    "lr": (1e-5, 1e-3),
}
# Other parameters to optimize:
# "max_epochs": (10, 100)
# "batch_size": (32, 128)

optimize_probe.optim_process = False
optimize_probe.init_points = 5
optimize_probe.n_iter = 50
optimize_probe.seed = 1

# Warning: these parameters are ignored when the the Bayesian optimization is enabled
SequenceMultiLabelClassificationProbe.num_layers = 2
SequenceMultiLabelClassificationProbe.hidden_size = 512
SequenceMultiLabelClassificationProbe.dropout = 0.2
SequenceMultiLabelClassificationProbe.lr = 0.0001

SequenceMultiLabelClassificationProbe.activation = "relu"
SequenceMultiLabelClassificationProbe.bias = True
SequenceMultiLabelClassificationProbe.num_labels = 50
SequenceMultiLabelClassificationProbe.labels = ""

# CosineAnnealing scheduler
CosineAnnealingCallback.warmup_steps = 2000
CosineAnnealingCallback.eta_min = 1e-7
