# general training parameters
train.wandb_params = {
    "project": "mtg-ssl",
    "offline": True,
    # NOTE: path to logs in the BSC cluster. Change it for local experiments
    "save_dir": "wandb",
    "entity": "mtg-upf",
}

# modules to use
train.representation = @melspectrogram
train.module = @maskingmodel
train.datamodule = @discotube
train.net = @transformer

# Lighting trainer parameters
train.params = {
    "accelerator": "gpu",
    "devices": 1,
    "max_steps": 300000,
    "log_every_n_steps": 50,
    "precision": "bf16-mixed",
    "strategy": "ddp_find_unused_parameters_true",
    "num_sanity_val_steps": 0,
}

# Dataloader
AudioDataset.num_frames = 48000 # 3s
AudioDataset.new_freq = 16000
AudioDataset.mono = True
AudioDataset.half_precision = True
AudioDataModule.num_workers = 0

# Discogs datamodule parameters
DiscotubeAudioDataModule.batch_size = 2
DiscotubeAudioDataModule.data_dir = "data/gtzan"
DiscotubeAudioDataModule.filelist_train = "data/gtzan/train_mmap.txt"
DiscotubeAudioDataModule.filelist_val = "data/gtzan/test_mmap.txt"

# CosineAnnealing scheduler
CosineAnnealingCallback.warmup_steps = 16000
CosineAnnealingCallback.eta_min = 1e-7

# MelSpectrogram parameters
nets.melspectrogram.MelSpectrogram.sr = 16000
nets.melspectrogram.MelSpectrogram.win_len = 512
nets.melspectrogram.MelSpectrogram.hop_len = 256
nets.melspectrogram.MelSpectrogram.power = 2
nets.melspectrogram.MelSpectrogram.n_mel = 96
nets.melspectrogram.MelSpectrogram.norm = "slaney"
nets.melspectrogram.MelSpectrogram.mel_scale = "slaney"
nets.melspectrogram.MelSpectrogram.norm_std = 1.268292820667291
nets.melspectrogram.MelSpectrogram.norm_mean = 2.06755686098554

# data augmentation
nets.melspectrogram.MelSpectrogram.stretch_factor = 1
nets.melspectrogram.MelSpectrogram.freq_mask_param = 10
nets.melspectrogram.MelSpectrogram.time_mask_param = 10

# MaskingModel parameters
modules.maskingmodel.MaskingModel.num_codebooks = 1
modules.maskingmodel.MaskingModel.lr = 0.001
modules.maskingmodel.MaskingModel.codebook_size = 4096
modules.maskingmodel.MaskingModel.mask_seconds = 0.4
modules.maskingmodel.MaskingModel.mask_prob = 0.6

# Transformer parameters
nets.transformer.Transformer.patch_size = (4, 16)
nets.transformer.Transformer.context_length = 63
nets.transformer.Transformer.in_chans = 1
nets.transformer.Transformer.embed_dim = 768
nets.transformer.Transformer.head_dims = 768
nets.transformer.Transformer.depth = 12
nets.transformer.Transformer.num_heads = 12
nets.transformer.Transformer.mlp_ratio=4.0
nets.transformer.Transformer.dropout=0.1
nets.transformer.Transformer.do_vit_tokenization = False
