# general training parameters
train.wandb_params = {
    "project": "mtg-ssl",
    "offline": True,
    # NOTE: path to logs in the BSC cluster. Change it for local experiments
    "save_dir": "wandb",
    "entity": "mtg-upf",
}

# modules to use
build_module.representation = @nets.melspectrogram.MelSpectrogram
build_module.module = @modules.maskingmodel.MaskingModel
build_module.net = @nets.transformer.Transformer

# Choose the devalopment dataloader
build_dev_datamodule.datamodule = @discotube
# Lighting trainer parameters
train.params = {
    "accelerator": "gpu",
    "devices": 1,
    "max_steps": 300000,
    "log_every_n_steps": 50,
    "precision": "bf16-mixed",
    "strategy": "ddp_find_unused_parameters_true",
    "num_sanity_val_steps": 0,
}

# Dataloader
AudioDataset.num_frames = 160000 # 3s
AudioDataset.orig_freq = 16000
AudioDataset.new_freq = 16000
AudioDataset.mono = True
AudioDataset.half_precision = True
AudioDataModule.num_workers = 0

# Discogs datamodule parameters
DiscotubeAudioDataModule.batch_size = 1
DiscotubeAudioDataModule.data_dir = "data/local"
DiscotubeAudioDataModule.filelist_train = "data/local/train_mmap.txt"
DiscotubeAudioDataModule.filelist_val = "data/local/test_mmap.txt"

# CosineAnnealing scheduler
CosineAnnealingCallback.warmup_steps = 30000
CosineAnnealingCallback.eta_min = 1e-7

# MelSpectrogram parameters
nets.melspectrogram.MelSpectrogram.sr = 16000
nets.melspectrogram.MelSpectrogram.win_len = 512
nets.melspectrogram.MelSpectrogram.hop_len = 256
nets.melspectrogram.MelSpectrogram.power = 2
nets.melspectrogram.MelSpectrogram.n_mel = 96
nets.melspectrogram.MelSpectrogram.norm = "slaney"
nets.melspectrogram.MelSpectrogram.mel_scale = "slaney"
nets.melspectrogram.MelSpectrogram.norm_std = 1.268292820667291
nets.melspectrogram.MelSpectrogram.norm_mean = 2.06755686098554

# data augmentation
nets.melspectrogram.MelSpectrogram.stretch_factor = 1
nets.melspectrogram.MelSpectrogram.freq_mask_param = 10
nets.melspectrogram.MelSpectrogram.time_mask_param = 10

# MaskingModel parameters
modules.maskingmodel.MaskingModel.num_codebooks = 1
modules.maskingmodel.MaskingModel.lr = 1e-4
modules.maskingmodel.MaskingModel.weight_decay = 1e-2
modules.maskingmodel.MaskingModel.codebook_size = 8192
modules.maskingmodel.MaskingModel.codebook_dim = 16
modules.maskingmodel.MaskingModel.mask_seconds = 0.4
modules.maskingmodel.MaskingModel.mask_prob = 0.6
modules.maskingmodel.MaskingModel.seed = 42
modules.maskingmodel.MaskingModel.plot_tokens = False

# Transformer parameters
nets.transformer.Transformer.patch_size = (96, 4)
nets.transformer.Transformer.in_chans = 1
nets.transformer.Transformer.embed_dim = 768
nets.transformer.Transformer.head_dims = 768
nets.transformer.Transformer.depth = 12
nets.transformer.Transformer.num_heads = 12
nets.transformer.Transformer.mlp_ratio=4.0
nets.transformer.Transformer.dropout=0.1
nets.transformer.Transformer.input_dropout=0.1
nets.transformer.Transformer.do_vit_tokenization = False
nets.transformer.Transformer.do_deepnorm = True
nets.transformer.Transformer.alpha_deepnorm = 0.1 # we can tune this number
